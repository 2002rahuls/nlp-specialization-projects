{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "605ec8c4",
   "metadata": {},
   "source": [
    "# RNN / GRU Language Model\n",
    "\n",
    "This notebook implements a **character-level language model** using **RNNs and GRUs** in TensorFlow. \n",
    "\n",
    "**Goals:**\n",
    "- Learn sequence modeling with GRU\n",
    "- Train a character-level language model\n",
    "- Generate text using temperature-controlled sampling\n",
    "- Evaluate log perplexity of the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8eef3d",
   "metadata": {},
   "source": [
    "Load Dataset and Prepare Vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e0cd3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique characters: 95\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "with open('data/input_text.txt', 'r',encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "vocab = sorted(list(set(text)))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "print(f\"Unique characters: {vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c8f342",
   "metadata": {},
   "source": [
    "Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5541691b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_to_tensor(line, vocab):\n",
    "    chars = list(line)\n",
    "    char2idx = {ch:i for i,ch in enumerate(vocab)}\n",
    "    ids = [char2idx[c] for c in chars]\n",
    "    return tf.convert_to_tensor(ids, dtype=tf.int64)\n",
    "\n",
    "def split_input_target(sequence):\n",
    "    return sequence[:-1], sequence[1:]\n",
    "\n",
    "def create_batch_dataset(lines, vocab, seq_length=100, batch_size=64):\n",
    "    text = \"\\n\".join(lines)\n",
    "    all_ids = line_to_tensor(text, vocab)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "    sequences = dataset.batch(seq_length+1, drop_remainder=True)\n",
    "    dataset_xy = sequences.map(lambda seq: split_input_target(seq))\n",
    "    dataset_xy = dataset_xy.shuffle(10000).batch(batch_size, drop_remainder=True)\n",
    "    return dataset_xy\n",
    "\n",
    "def temperature_random_sampling(logits, temperature=1.0):\n",
    "    logits = logits / temperature\n",
    "    probs = tf.nn.softmax(logits, axis=-1)\n",
    "    return tf.random.categorical(tf.math.log(probs), num_samples=1)[-1,0]\n",
    "\n",
    "def text_from_ids(ids, vocab):\n",
    "    return ''.join([vocab[i] for i in ids.numpy().flatten()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5fa955",
   "metadata": {},
   "source": [
    "GRULM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a71cc234",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRULM(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim=256, rnn_units=128):\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(rnn_units, return_sequences=True, return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size, activation='log_softmax')\n",
    "\n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        x = self.embedding(inputs, training=training)\n",
    "        if states is None:\n",
    "            states = self.gru.get_initial_state(x)\n",
    "        x, states = self.gru(x, initial_state=states, training=training)\n",
    "        x = self.dense(x, training=training)\n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "def compile_model(model, learning_rate=0.00125):\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=loss)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cc292f",
   "metadata": {},
   "source": [
    "Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12e1613",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = text.split('\\n')\n",
    "seq_length = 100\n",
    "batch_size = 64\n",
    "\n",
    "dataset = create_batch_dataset(lines, vocab, seq_length=seq_length, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3238d09",
   "metadata": {},
   "source": [
    "Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adda6d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "246/246 [==============================] - 40s 147ms/step - loss: 2.6609\n",
      "Epoch 2/10\n",
      "246/246 [==============================] - 44s 175ms/step - loss: 2.0413\n",
      "Epoch 3/10\n",
      "246/246 [==============================] - 47s 188ms/step - loss: 1.8450\n",
      "Epoch 4/10\n",
      "246/246 [==============================] - 49s 196ms/step - loss: 1.7409\n",
      "Epoch 5/10\n",
      "246/246 [==============================] - 62s 248ms/step - loss: 1.6754\n",
      "Epoch 6/10\n",
      "246/246 [==============================] - 48s 191ms/step - loss: 1.6303\n",
      "Epoch 7/10\n",
      "246/246 [==============================] - 46s 184ms/step - loss: 1.5978\n",
      "Epoch 8/10\n",
      "246/246 [==============================] - 45s 181ms/step - loss: 1.5727\n",
      "Epoch 9/10\n",
      "246/246 [==============================] - 47s 187ms/step - loss: 1.5529\n",
      "Epoch 10/10\n",
      "246/246 [==============================] - 44s 176ms/step - loss: 1.5372\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 256\n",
    "rnn_units = 128\n",
    "\n",
    "model = GRULM(vocab_size, embedding_dim, rnn_units)\n",
    "model = compile_model(model)\n",
    "\n",
    "history = model.fit(dataset, epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0aff5db",
   "metadata": {},
   "source": [
    "Evaluate Log Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ef1f8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Perplexity: 1.4424704\n"
     ]
    }
   ],
   "source": [
    "def log_perplexity(preds, target, padding_id=1):\n",
    "    # Mask padding (if any)\n",
    "    mask = tf.cast(tf.not_equal(target, padding_id), tf.float32)\n",
    "    log_probs = tf.reduce_sum(tf.one_hot(target, preds.shape[-1]) * tf.math.log(tf.nn.softmax(preds, axis=-1)), axis=-1)\n",
    "    log_probs = log_probs * mask\n",
    "    return -tf.reduce_mean(log_probs)\n",
    "\n",
    "# Example evaluation\n",
    "for input_batch, target_batch in dataset.take(1):\n",
    "    preds, _ = model(input_batch, return_state=True)\n",
    "    lp = log_perplexity(preds, target_batch)\n",
    "    print(\"Log Perplexity:\", lp.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d172d8",
   "metadata": {},
   "source": [
    "Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4db888d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a great things see him that I should this thou must be love me to me, sir, the world in all the company.\n",
      "\n",
      "HAMLET.\n",
      "Iâ€™ll see his common, let me so make the money they have all the son age and the shall be more\n"
     ]
    }
   ],
   "source": [
    "class GenerativeModel:\n",
    "    def __init__(self, model, vocab, temperature=1.0):\n",
    "        self.model = model\n",
    "        self.vocab = vocab\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def generate_one_step(self, inputs, states=None):\n",
    "        input_ids = line_to_tensor(inputs, self.vocab)\n",
    "        input_ids = tf.expand_dims(input_ids, 0)[:, -1:]  # last token\n",
    "        logits, states = self.model(input_ids, states=states, return_state=True, training=False)\n",
    "        logits = logits[:, -1, :]\n",
    "        next_id = temperature_random_sampling(logits, self.temperature)\n",
    "        next_char = text_from_ids(tf.expand_dims(next_id,0), self.vocab)\n",
    "        return next_char, states\n",
    "\n",
    "    def generate_n_chars(self, num_chars, prefix):\n",
    "        states = None\n",
    "        next_char = prefix\n",
    "        result = [prefix]\n",
    "        for _ in range(num_chars):\n",
    "            next_char, states = self.generate_one_step(next_char, states)\n",
    "            result.append(next_char)\n",
    "        return \"\".join(result)\n",
    "\n",
    "gen_model = GenerativeModel(model, vocab, temperature=0.5)\n",
    "print(gen_model.generate_n_chars(200, prefix=\"I have a great\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b6d510",
   "metadata": {},
   "source": [
    "Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d88d49ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/grulm_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/grulm_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully saved at: saved_model/grulm_model\n"
     ]
    }
   ],
   "source": [
    "# Directory to save the model\n",
    "save_dir = \"saved_model/grulm_model\"\n",
    "import os\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save the model in TensorFlow SavedModel format\n",
    "model.save(save_dir)\n",
    "\n",
    "print(f\"Model successfully saved at: {save_dir}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
